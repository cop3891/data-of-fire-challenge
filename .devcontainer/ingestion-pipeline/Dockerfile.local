# Use an official OpenJDK runtime as a parent image.
FROM openjdk:8-jdk-slim

# Set an environment variable for the version of Spark (optional)
ENV SPARK_VERSION 3.4.1

# Install Python3, pip and other necessary packages.
RUN apt-get update \
    && apt-get install -y \
    python3 \
    python3-pip \
    bash \
    curl \
    procps \
    && rm -rf /var/lib/apt/lists/*

# Optionally, you can install any extra dependencies (e.g., vim, git, etc.)
# RUN apt-get update && apt-get install -y vim git && rm -rf /var/lib/apt/lists/*

# Create a working directory inside the container
WORKDIR /app

RUN pip install ipykernel

# Copy the requirements file and install dependencies
COPY ./ingestion-pipeline/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Expose a port if your Spark application or web UI needs it (e.g., Spark WebUI default: 4040)
EXPOSE 4040

# Set the default command to run your PySpark job/script
# Update "your_script.py" to the name of your script.
CMD ["tail", "-f", "/dev/null"]